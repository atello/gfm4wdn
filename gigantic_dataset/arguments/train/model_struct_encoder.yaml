model_configs:
- !!python/object:gigantic_dataset.utils.configs.ModelConfig
    weight_path: ""
    nc: 32
    name:  GFM
    num_layers: 5
    act: relu # unfunctional
    has_final_linear: false # unfunctional
lr: 0.0005
weight_decay: 0.00001
epochs: 500
mask_rate: 0.5
criterion: mse
batch_size: 64  # this can be 120 in habrok
use_data_batch: true
device: cuda
norm_type: znorm
norm_on:
    - node
    - edge
task: semi
log_method: "wandb"
project_name: "gfm-wdn_train" #it is not auto-change when you eval(.)
save_path: '' # make it empty to auto-gen in a first training
log_per_epoch: 1
run_prefix: "GFM-head-degrees"  # to distinguish two wandb runs