model_configs:
- !!python/object:gigantic_dataset.utils.configs.ModelConfig
    weight_path: "/home/andres/Dropbox/PhD Smart Environments - RUG/ExternalProjects/WDN_datasets/gfm-wdn/model_logs/29WDNs_experiments/GFM-struct-bias-weightedLoss+29wdns+DualEncoderGFM+20250828_185115_1748178/best_DualEncoderGFM_0.pt"
    num_layers: 5  # in habrok should be 15
    nc: 32
    name: DualEncoderGFM
    act: relu # unfunctional
    has_final_linear: false # unfunctional
lr: 0.0005
weight_decay: 0.00001
epochs: 150
mask_rate: 0.95
criterion: mse
batch_size: 64  # this can be 120 in habrok
use_data_batch: false
device: cuda
norm_type: znorm
norm_on:
    - node
    - edge
task: semi
log_method: "wandb"
project_name: "gfm-wdn_train" #it is not auto-change when you eval(.)
save_path: '' # make it empty to auto-gen in a first training
log_per_epoch: 1
run_prefix: "DualEncoderGFM-FT-cos-wLoss-10k-150epochs"  # to distinguish two wandb runs